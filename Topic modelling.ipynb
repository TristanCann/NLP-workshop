{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9039b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bertopic\n",
    "import pandas as pd\n",
    "import sentence_transformers\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aaac1",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "Topic modelling is the process of grouping large volumes of text into *topics*, that is collections of texts that align in the words that they use. At the conceptual level, topic modelling tries to cluster representations of the texts based on some similarity metric. As a result, there are many different approaches and techniques for topic modelling.\n",
    "\n",
    "For the purposes of this workshop we will use BERTopic, a method that leverages the embeddings that are discussed in one of the other tutorials. This example will use the same news content about `climate change` and `global warming` as the other examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d906d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b827a0",
   "metadata": {},
   "source": [
    "At the simplest level, the BERTopic library handles both the embedding and clustering steps using its default parameters. We collect topic labels and probability assignments for each text in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8d5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = bertopic.BERTopic()\n",
    "topics, probs = topic_model.fit_transform(df_news.title[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7d8fe",
   "metadata": {},
   "source": [
    "How do we interpret this information? The topic label denotes which group a given text belongs to. Note that the topic `-1` is included as a catch-all for any texts that do not fit within a clear topic, and as such should not be considered as cohesive topic. The probabilities indicate a measure of confidence in the topic model that this is the correct topic assignment for a given text. This can vary significantly between texts.\n",
    "\n",
    "These labels and probablities are useful, but they don't tell us much about the texts. Luckily, BERTopic automatically processes summary information for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148a5dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics = topic_model.get_topic_info()\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b34454",
   "metadata": {},
   "source": [
    "This set of topic information tells us all we need to know about the clusters forming around texts. `Count` indicates the number of texts assigned to this topic, `Representation` leverages TF-IDF to rank the key terms that characterise a topic and `Representative_Docs` returns the texts closest to the topic's TF-IDF representation. These representative words and docs are typically enough to characterise the largest topics.\n",
    "\n",
    "In the example above, we can see topics forming around Pope Francis, Canadian politics and Indian PM Modi arriving at COP26. Many of the topics reference COP26, a key event at the time this data was produced.\n",
    "\n",
    "Topic size is an important factor when considering the quality of a topic. If a topic is small relative to the size of your dataset it is more likely to have formed around highly specfic cases or anomalies in the data. Here the smallest topic is approximately 1\\% of the dataset at 11 documents. Normally, we can avoid these issues with topic size by considering only the largest topics for subsequent analysis, such as the top 10.\n",
    "\n",
    "A major strength of the BERTopic method is how customisable it is with different embedding components. We can pass any embeddings we want to the model to check their effect on the topics we find. Here we'll try the `all-distilroberta-v1` model, but you can find a list of pre-trained models in the [documentation](https://www.sbert.net/docs/sentence_transformer/pretrained_models.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c182bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = sentence_transformers.SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "roberta_topic_model = bertopic.BERTopic(embedding_model=sentence_model)\n",
    "rob_topics, rob_probs = roberta_topic_model.fit_transform(df_news.title[:1000])\n",
    "df_roberta_topics = roberta_topic_model.get_topic_info()\n",
    "df_roberta_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5d5e3",
   "metadata": {},
   "source": [
    "We can see that some of the detected topics are consistent between different embedding models, whereas others have changed. Canadian politics, Boris Johnson and Pope Francis are common catalysts, whereas others such as COP26 are captured differently. Comparing topics across different model parameters can be a good way to assess their quality. Those that are consistent are likely to be more robust and clearly defined in your data.\n",
    "\n",
    "At present, BERTopic recomputes the embeddings each time the model is fit. This can be resource intensive (in both time and CPU cycles) when applied to large datasets. You can instead pass precomputed embeddings to the model, in which case BERTopic only looks for the best way to group texts together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0ddfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embs = sentence_model.encode(df_news.title[:1000])\n",
    "\n",
    "roberta_topic_model = bertopic.BERTopic(embedding_model=sentence_model)\n",
    "rob_topics, rob_probs = roberta_topic_model.fit_transform(df_news.title[:1000],embs)\n",
    "df_roberta_topics = roberta_topic_model.get_topic_info()\n",
    "df_roberta_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb7e53",
   "metadata": {},
   "source": [
    "Once again, we can see some variation in the topics that have been returned - even though we are passing the same data and  embedding model. This is because BERTopic has stochastic elements. To fix the random state for reproducibility purposes, we need to customise the clustering and dimensionality reduction models.\n",
    "\n",
    "By default, BERTopic uses the `HDBSCAN` model for clustering. We can define the model ourselves to alter the clustering parameters - review the [HDBSCAN documentation](https://hdbscan.readthedocs.io/en/latest/) for a full list of options. We'll also set a global seed in `numpy` to fix the random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = bertopic.BERTopic(hdbscan_model=hdbscan_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e768d",
   "metadata": {},
   "source": [
    "It is possible to substitute one of a number of clustering algorithms into BERTopic - the [documentation](https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html) gives a list of examples.\n",
    "\n",
    "The final component of BERTopic we can vary is the dimensionality reduction step. This process reduces some of the complexity in the model and make it more memory efficient (as noted when we discussed the value of sentence embeddings over bag of words and TF-IDF methods). Like with the embedding and clustering methods there are many options that can be easily subsituted, or updated for finer control of the method, with exmaples listed in the [documentation](https://maartengr.github.io/BERTopic/getting_started/dim_reduction/dim_reduction.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8225738",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "topic_model = bertopic.BERTopic(umap_model=umap_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef043d",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Fit a topic model over the first 1000 article bodies using the default BERTopic settings. Find the list of row numbers in the dataset that are about the president of Zimbabwe according to the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e557c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558d3a02",
   "metadata": {},
   "source": [
    "The topics and probabilities assigned by BERTopic correspond to the best fit. Using the probabilities return for each text assigned to the topics in your previous model, find the topics with the highest and lowest mean probability. Do these values tell you anything about the topics highlighted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9b0660",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
