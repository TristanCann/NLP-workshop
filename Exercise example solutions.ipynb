{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import bertopic\n",
    "import spacy\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d384809",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1908d1",
   "metadata": {},
   "source": [
    "# Text cleaning and searching\n",
    "\n",
    "How many of the headlines mention Justin Trudeau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56be8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We'll consider a headline as relevant if it mentions just his surname.\n",
    "count = 0\n",
    "for t in df_news.title:\n",
    "    if 'trudeau' in t.lower():\n",
    "        count += 1\n",
    "print(f'{count} titles mention Trudeau.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd2b0e",
   "metadata": {},
   "source": [
    "How many of the headlines mention one of the G7 countries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be8e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This query is not an exhaustive list - you could do more to match the UK and US.\n",
    "query = r'(Canada|Italy|France|Germany|Japan|United Kingdom|UK|United States|US|)'\n",
    "count = 0\n",
    "for t in df_news.title:\n",
    "    if np.any([m != '' for m in re.findall(query,t)]):\n",
    "        count += 1\n",
    "print(f'{count} titles mention a G7 country.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6fd51d",
   "metadata": {},
   "source": [
    "# Vectorisation and embeddings\n",
    "\n",
    "Find the five most similar headline among the first 1000 headlines to the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0784c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_model = sentence_transformers.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "ref_text = ['President Biden visited other G7 leaders in London']\n",
    "sents = ref_text + [t for t in df_news.title[:1000]]\n",
    "embs = sentence_model.encode(sents,show_progress_bar=True)\n",
    "\n",
    "## We can calculate pairwise similarity easily.\n",
    "sim = cosine_similarity(embs)\n",
    "## The first row gives the similarity scores we care about. Find the maximal elements.\n",
    "matches = np.argsort(sim[0])\n",
    "for i in matches[-6:-1]:\n",
    "    print(sents[i],sim[0,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6fc55",
   "metadata": {},
   "source": [
    "Which of the first 1000 articles uses *climate* the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X_1k = vectorizer.fit_transform([t for t in df_news.body[:1000]])\n",
    "\n",
    "## We need to get the feature names from vectorizer to find out which column of X_1k to check.\n",
    "clim_col = list(vectorizer.get_feature_names_out()).index('climate')\n",
    "\n",
    "max_doc = np.argmax(X_1k[:,clim_col])\n",
    "print(f'The {max_doc}th article uses climate {X_1k[max_doc,clim_col]} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47936eb9",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "Fit a topic model over the first 1000 article bodies using the default BERTopic settings. Find the list of row numbers in the dataset that are about the president of Zimbabwe according to the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0050a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = bertopic.BERTopic()\n",
    "topics, probs = topic_model.fit_transform(df_news.body[:1000])\n",
    "df_topics = topic_model.get_topic_info()\n",
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b5717",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note the that the topic label here is dependent on a particular iteration of the topic model (unless you fix the random state)\n",
    "## The topic number should be around 25 in most cases however.\n",
    "print(f'Row ids corresponding to the president of Zimbabwe are: {[i for i,t in enumerate(topics) if t == 25]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3ebb99",
   "metadata": {},
   "source": [
    "The topics and probabilities assigned by BERTopic correspond to the best fit. Using the probabilities return for each text assigned to the topics in your previous model, find the topics with the highest and lowest mean probability. Do these values tell you anything about the topics highlighted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b70993",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we convert topics and probs to a more convenient format.\n",
    "## This gives a list of probabilities for each topic label.\n",
    "topic_probs = {}\n",
    "for t,p in zip(topics,probs):\n",
    "        if not topic_probs.get(t):\n",
    "            topic_probs[t] = []\n",
    "        topic_probs[t].append(p)\n",
    "\n",
    "## Next we average them and record the maximum and minimum we've seen.\n",
    "max_avg_topic = -2\n",
    "min_avg_topic = -2\n",
    "max_avg = 0\n",
    "min_avg = 1\n",
    "for t in topic_probs:\n",
    "    avg = np.mean(topic_probs[t])\n",
    "    if avg > max_avg:\n",
    "        max_avg_topic = t\n",
    "        max_avg = avg\n",
    "    if avg < min_avg:\n",
    "        min_avg_topic = t\n",
    "        min_avg = avg\n",
    "\n",
    "## This is almost certainly going to be topic -1 with probability 0 - BERTopic doesn't record probabilities for the junk topic.\n",
    "## This is because they don't really make sense.\n",
    "print(f'The topic with lowest average probability is {min_avg_topic} with probability {min_avg}.')\n",
    "## This is probably one of the smaller topics with a very high probability. Be careful of overfitting to duplicate or\n",
    "## very similar texts in your dataset causing the model to converge anomalously.\n",
    "print(f'The topic with highest average probability is {max_avg_topic} with probability {max_avg}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9f791",
   "metadata": {},
   "source": [
    "# Text classification\n",
    "\n",
    "Find the 10 most common entities across the first 1000 articles. Hint: you may find the `collections` library useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e3037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import tqdm  ## This displays a progress bar to track efficiency.\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ents = []\n",
    "docs = []\n",
    "for i, row in tqdm.tqdm(df_news[:1000].iterrows()):\n",
    "    doc = nlp(row.body)\n",
    "    docs.append(doc)  ## Saving these to avoid repeating the calculation.\n",
    "    ents += [e.text for e in doc.ents]  ## Note that taking the text is required, otherwise the entities will not be matched.\n",
    "ent_counts = collections.Counter(ents)\n",
    "print(f'The ten most common entities are {ent_counts.most_common(10)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d62068e",
   "metadata": {},
   "source": [
    "Different part of speech tags will appear with different frequencies. Find the articles among the first 1000 articles that that have the highest and lowest proportion of proper nouns in their bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ddeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prop = 0\n",
    "min_prop = 1\n",
    "max_prop_id = -1\n",
    "min_prop_id = -1\n",
    "\n",
    "for i,doc in enumerate(docs):\n",
    "    pns = 0\n",
    "    for t in doc:\n",
    "        if t.pos_ == 'PROPN':\n",
    "            pns += 1\n",
    "    prop = pns/len(doc)\n",
    "    \n",
    "    if prop > max_prop:\n",
    "        max_prop = prop\n",
    "        max_prop_id = i\n",
    "    if prop < min_prop:\n",
    "        min_prop = prop\n",
    "        min_prop_id = i\n",
    "\n",
    "print(f'Article {min_prop_id} has the lowest proportion of proper nouns: {min_prop}.')\n",
    "print(f'Article {max_prop_id} has the lowest proportion of proper nouns: {max_prop}.') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552aa98",
   "metadata": {},
   "source": [
    "News media is sometimes criticised as focused on negative stories. How are the sentiment labels and class probabilities split over the first 1000 headlines? Does this trend tell you anything about the classes or your chosen model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9cddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "## The example model used earlier in these notes doesn't give a range of sentiment scores and rarely assigns neutral.\n",
    "## Instead it gives a positive or negative label with a probability of being positive or negative.\n",
    "## We should use a different model - the Cardiff NLP group is a good source.\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\",model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "sents = [sentiment_analysis(t)[0] for t in df_news[:1000].title]\n",
    "\n",
    "## Each score is a dict, label and probability. We need to split them into pos, neu, neg.\n",
    "pos,neu,neg = [],[],[]\n",
    "for s in sents:\n",
    "    if s['label'] == 'positive':\n",
    "        pos.append(s['score'])\n",
    "    elif s['label'] == 'neutral':\n",
    "        neu.append(s['score'])\n",
    "    else:\n",
    "        neg.append(s['score'])\n",
    "\n",
    "print(f'Proportion of positive headlines: {len(pos)/1000}')\n",
    "print(f'Proportion of neutral headlines: {len(neu)/1000}')\n",
    "print(f'Proportion of negative headlines: {len(neg)/1000}')\n",
    "\n",
    "plt.hist(pos,label='Positive',density=True)\n",
    "plt.hist(neu,label='Neutral',density=True)\n",
    "plt.hist(neg,label='Negative',density=True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "## We see that most of the headlines are reported as neutral, with slightly more negative than positive.\n",
    "## The model may be less certain of the negative labels (some slight skew in the distribution of probabilities).\n",
    "## This may be an indication that it is conflating 'neutral' and 'not sure', but this is not conclusive.\n",
    "## Analysis based on neutral labels should therefore be considered carefully to ensure there are no confounding factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a6d00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
