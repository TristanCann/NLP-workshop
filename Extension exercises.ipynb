{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49935ac-cb9e-4aad-81ce-014ce6749e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import bertopic\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import string\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecaf16c-f2a5-4206-935c-1f6187a0b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')\n",
    "\n",
    "# For testing purposes, you might find it helpful to cut this dataset down to only the first few articles.\n",
    "df_news = df_news.iloc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994081ec-154e-4d4d-98c9-054ec782fd8c",
   "metadata": {},
   "source": [
    "## Challenge 1: Identifying characteristic words.\n",
    "One of the most valuable applications of TF-IDF is to use the IDF weighting to provide an explainable way to highlight the terms that are key to a given document. This can even be done across a group of documents, such as all those articles that are produced by the same news source.\n",
    "\n",
    "Explore the articles present in the provided example data and determine the five most characteristic words for each source. You should:\n",
    "1. Identify the total list of sources in the dataset.\n",
    "2. Group the article texts together to create a document for each source.\n",
    "3. Calculate the TF-IDF vectors for each source. Consider the use of custom stopword sets.\n",
    "4. Identify the tokens with maximum weight in each vector.\n",
    "\n",
    "Hint: TF-IDF methods include two parameters that affect the results: `max_df` and `min_df`. Review how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb45066a-cad6-4bc6-a6ab-87953ab819d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14a8457d-9ede-4e48-a82b-b8b7aa5995fc",
   "metadata": {},
   "source": [
    "## Challenge 2: n-gram construction.\n",
    "Words are not the only sub-sentence unit used to study linguistic patterns in natural language processing. A more general unit called an `n-gram` defines sequences of `n` consecutive words extracted from a text. For example, `2-grams` or `bigrams` are pairs of words appearing consecutively in a given string. More concretely, given the text `This is example text` would return the following bigrams: `this is`, `is example`, `example text`.\n",
    "\n",
    "Using different values of `n`, look for different n-gram patterns appearing the the titles and body text for the provided example dataset. You should:\n",
    "1. Write a custom function to calculate all n-grams of a given string. This function should include optional stopword removal.\n",
    "2. Compare the bigram frequency values both before and after removing stopwords from the text.\n",
    "3. Find the most frequent 4-grams that include `climate` and the most frequent 4-grams that include `change`. Determine the overlap between these sets of 4-grams (i.e. those that include both `climate` and `change`.\n",
    "4. Find the the set of 5-grams that contain `climate`. After partitioning the example dataset into one-day windows, can you see any patterns in how the usage of `climate` varies over time? Hint: you will need a larger sample or different sampling strategy to see more than one day included in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64fee8-1fa9-4aa9-bf66-0aeec754c792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de88e800-ed63-45a0-a990-a7d8fff5fac0",
   "metadata": {},
   "source": [
    "## Challenge 3: Narrative consistency.\n",
    "Text data can be difficult to work with for many reasons, mainly due to it being noisy and needing careful cleaning. The example data you were given contains a particular issue with portions of the text that may be apparently unrelated to the major themes of the text. This is an inevitable consequence of the data source; it is provided by a service that collects and formats online news articles and this sometimes includes text from web features (e.g. other story links) being incorrectly included in the main text. Luckily, we can use some of the techniques we introduced previously to look for divergence in the semantic content of portions of the text.\n",
    "\n",
    "Look for evidence of narrative inconsistency in the body text of the example data. You should:\n",
    "1. Determine an appropriate unit of analysis to partition the article into, and preprocess the data into these units.\n",
    "2. Describe an algorithm to measure the internal narrative consistency of an article that uses these sub-article units.\n",
    "3. Apply this algorithm to quantify the narrative consistency of all articles in the example dataset. Which articles are the most and least narratively consistent?\n",
    "\n",
    "Hint: Consider mapping the semantic space covered by the article. Which articles cover the largest or smallest semantic space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3ca83-ba96-4857-8559-89469c81f538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9863f6ac-a577-4a34-b376-06eaa0b765a6",
   "metadata": {},
   "source": [
    "## Challenge 4: Positivity and negativity in narratives.\n",
    "The precise dataset and seeding applied to topic modelling can make a big difference to the results. Sometimes this can mean that the detected topics may miss some of the smaller, more nuanced themes. This means it is often a good idea to consider a few different outputs from topic modelling under a range of paramter values. At this stage, confidence in the model outputs can be determined by aggregate the common patterns appearing across different runs.\n",
    "\n",
    "Compare the topics identified in different perspectives on the example data. You should:\n",
    "1. Apply the BERTopic framework with several different parameter combinations in the UMAP and HDBSCAN processes. Do you notice any topics that are consistent over runs?\n",
    "2. Partition the news dataset into three sets of articles based on valence (i.e. positive, neutral, negative). What proportion of articles fall in each category? How sensitive is this value to the method of sentiment analysis applied?\n",
    "3. Visualise the semantic space for the entire corpus. Are there any spatial patterns emerging based on the article valence?\n",
    "4. Apply BERTopic to each subset. Compare the topics found on each subet of the dataset to those found over all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dd84a4-0ce9-4868-9995-179e0717a6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
