{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49935ac-cb9e-4aad-81ce-014ce6749e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentence_transformers\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import bertopic\n",
    "import spacy\n",
    "from transformers import pipeline\n",
    "import string\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ecaf16c-f2a5-4206-935c-1f6187a0b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')\n",
    "\n",
    "# For testing purposes, you might find it helpful to cut this dataset down to only the first 500 articles.\n",
    "df_news = df_news.iloc[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994081ec-154e-4d4d-98c9-054ec782fd8c",
   "metadata": {},
   "source": [
    "## Challenge 1: Identifying characteristic words.\n",
    "One of the most valuable applications of TF-IDF is to use the IDF weighting to provide an explainable way to highlight the terms that are key to a given document. This can even be done across a group of documents, such as all those articles that are produced by the same news source.\n",
    "\n",
    "Explore the articles present in the provided example data and determine the five most characteristic words for each source. You should:\n",
    "1. Identify the total list of sources in the dataset.\n",
    "2. Group the article texts together to create a document for each source.\n",
    "3. Calculate the TF-IDF vectors for each source. Consider the use of custom stopword sets.\n",
    "4. Identify the tokens with maximum weight in each vector.\n",
    "\n",
    "Hint: TF-IDF methods include two parameters that affect the results: `max_df` and `min_df`. Review how they affect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb45066a-cad6-4bc6-a6ab-87953ab819d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['haidagwaiiobserver.com' 'worldbank.org' 'eurasiareview.com'\n",
      " 'stettlerindependent.com' 'reliefweb.int' 'bloombergquint.com'\n",
      " 'catholicnewsagency.com' 'cranbrooktownsman.com' 'dailyecho.co.uk'\n",
      " 'conservativeangle.com']\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "#1. Get the list of all sources.\n",
    "# The source column has most of this, but we need to process the data a little first.\n",
    "df_news['source_uri'] = [u['uri'] for u in df_news.source]  ## Alternatively, set() could be applied to the list comprehension, but it's useful to save this.\n",
    "unique_sources = df_news['source_uri'].unique()\n",
    "print(unique_sources[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9140cb4-7a3a-42ed-a930-8a20d9484fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 340/340 [00:00<00:00, 685.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles is expected to call for a \"vast military-style campaign\" to address urgent environmental iss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#2. Group the article texts for each source into one document.\n",
    "# For now we'll record this in a dictionary domain: string to keep the link between source and the full text.\n",
    "source_text = {}\n",
    "for u in tqdm.tqdm(unique_sources):  # tqdm is very handy to add a progress bar and indicate roughly how long it will take to execute this for loop.\n",
    "    df_u = df_news[df_news.source_uri==u]\n",
    "    source_text[u] = ' '.join(df_u.body)\n",
    "print(source_text['dailyecho.co.uk'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bd21bee-f167-4f5d-8502-21e6ea7e20f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000' '10' '100' ... 'zero' 'zeshan' 'zone']\n",
      "[[0.         0.         0.         ... 0.15650129 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.02345739 0.         0.         ... 0.10601783 0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.07375629 ... 0.03991311 0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.07159617 ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#3. Calculate the TF-IDF vectors for the sources. Consider the use of custom stop words.\n",
    "# We should first define our list of stopwords, then we can just apply the example code we saw before.\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "stop = list(_stop_words.ENGLISH_STOP_WORDS) + ['climate', 'change', 'global', 'warming']  ## These terms should be removed as they're part of our search terms.\n",
    "stop += ['comment', 'subscribe', 'cookie', 'accept', 'reject']  ## Add in some terms that might appear from online data quirks (not at all an exhaustive list).\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(stop_words=stop,max_df=0.5,min_df=10) ## Using max_df and min_df we automatically ignore terms in more than half of all sources and in fewer than 10 sources.\n",
    "X_tf = tf_vectorizer.fit_transform([source_text[u] for u in unique_sources])  ## We need to use this list comprehension to ensure that the order matches.\n",
    "print(tf_vectorizer.get_feature_names_out())\n",
    "print(X_tf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c83459c-7f9b-4d01-b734-1cdeaf0c1cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['london', 'health', 'pollution', 'ride', 'air']\n",
      "['policy', 'project', 'bank', 'disaster', 'risk']\n",
      "['democrats', 'white', 'interested', 'biden', 'americans']\n"
     ]
    }
   ],
   "source": [
    "#4. Identify the tokens with maximum weight in each document.\n",
    "# Each row in X_tf corresponds to a particular source, whereas the columns refer to the feature names (i.e. words).\n",
    "feature_names = tf_vectorizer.get_feature_names_out()\n",
    "\n",
    "source_key_terms = {}\n",
    "for i,u in enumerate(unique_sources):\n",
    "    max_cols = np.argsort(X_tf[i].toarray())  ## We need to make a dense array for the sorting function to work.\n",
    "    source_key_terms[u] = [feature_names[j] for j in max_cols[0,-5:]]\n",
    "\n",
    "print(source_key_terms['dailyecho.co.uk'])\n",
    "print(source_key_terms['worldbank.org'])\n",
    "print(source_key_terms['conservativeangle.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "251d52fe-a376-4097-97e5-2d0f3bb76b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see from checking through these key terms that we can begin to characterise the sources based on the types of words that they prefer to use.\n",
    "# This can reveal elements of topic focus or region of interest or political leaning from even just a few most characteristic terms.\n",
    "# This is an application that really benefits from having a big dataset. You need to see a lot of content to start getting enough term frequency to \n",
    "# see the most important terms.\n",
    "# One key thing that should be clear here is the explainability of TF-IDF. We can link our statistical results back to the very tokens we started with.\n",
    "# This is a key advantage of these simpler methods over transformers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a8457d-9ede-4e48-a82b-b8b7aa5995fc",
   "metadata": {},
   "source": [
    "## Challenge 2: n-gram construction.\n",
    "Words are not the only sub-sentence unit used to study linguistic patterns in natural language processing. A more general unit called an `n-gram` defines sequences of `n` consecutive words extracted from a text. For example, `2-grams` or `bi-grams` are pairs of words appearing consecutively in a given string. More concretely, given the text `This is example text` would return the following bi-grams: `this is`, `is example`, `example text`.\n",
    "\n",
    "Using different values of `n`, look for different n-gram patterns appearing the the titles and body text for the provided example dataset. You should:\n",
    "1. Compare the bi-gram frequency values both before and after removing stopwords from the text.\n",
    "2. Find the most frequent 4-grams that include `climate` and the most frequent 4-grams that include `change`. Determine the overlap between these sets of 4-grams (i.e. those that include both `climate` and `change`.\n",
    "3. Find the the set of 5-grams that contain `climate change`. After partitioning the example dataset into one-day windows, can you see any patterns in how the usage of `climate change` varies over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea64fee8-1fa9-4aa9-bf66-0aeec754c792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de88e800-ed63-45a0-a990-a7d8fff5fac0",
   "metadata": {},
   "source": [
    "## Challenge 3: Narrative consistency.\n",
    "Text data can be difficult to work with for many reasons, mainly due to it being noisy and needing careful cleaning. The example data you were given contains a particular issue with portions of the text that may be apparently unrelated to the major themes of the text. This is an inevitable consequence of the data source, it is provided by a service that collects and formats online news articles and this sometimes includes text from web features (e.g. other story links) being incorrectly inlcuded in the main text. Luckily, we can use some of the techniques we introduced previously to look for divergence in the semantic content of portions of the text.\n",
    "\n",
    "Look for evidence of narrative inconsistency in the body text of the example data. You should:\n",
    "1. Determine an appropriate unit of analysis to partition the article into, and preprocess the data into these units.\n",
    "2. Describe an algorithm to measure the internal narrative consistency of an article that uses these sub-article units.\n",
    "3. Apply this algorithm to quantify the narrative consistency of all articles in the example dataset. Which articles are the most and least narratively consistent?\n",
    "\n",
    "Hint: Consider mapping the semantic space covered by the article. Which articles cover the largest or smallest semantic space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee3ca83-ba96-4857-8559-89469c81f538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9863f6ac-a577-4a34-b376-06eaa0b765a6",
   "metadata": {},
   "source": [
    "## Challenge 4: Positivity and negativity in narratives.\n",
    "The precise dataset and seeding applied to topic modelling can make a big difference to the results. Sometimes this can mean that the detected topics may miss some of the smaller, more nuanced themes. This means it is often a good idea to consider a few different outputs from topic modelling under a range of paramter values. At this stage, confidence in the model outputs can be determined by aggregate the common patterns appearing across different runs.\n",
    "\n",
    "Compare the topics identified in different perspectives on the example data. You should:\n",
    "1. Apply the BERTopic framework with several different parameter combinations in the UMAP and HDBSCAN processes. Do you notice and topics that are consistent over runs?\n",
    "2. Partition the news dataset into three sets of articles based on valence (i.e. positive, neutral, negative). What proportion of articles fall in each category? How sensitive is this value to the method of sentiment analysis applied?\n",
    "3. Visualise the semantic space for the entire corpus. Are there any spatial patterns emerging based on the article valence?\n",
    "4. Apply BERTopic to each subset. Compare the topics found on each subet of the dataset to those found over all texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c8da26-851b-44e9-bace-faa44c411c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
