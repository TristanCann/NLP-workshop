{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63d2af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c1365",
   "metadata": {},
   "source": [
    "# Text preprocessing\n",
    "\n",
    "Text data often needs preprocessing before analysis can take place. This can account for potential problems like inconsistencies in text formatting, typos and normalisation of punctuation and case.\n",
    "\n",
    "In the next cell, we introduce some example data for this workshop, a collection of online news articles from October 2021 that use either of the terms `climate change` or `global warming`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e01f840",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File data/cc_gw_news_blogs_2021-10-01_2021-10-31.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_news = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/cc_gw_news_blogs_2021-10-01_2021-10-31.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\NLP-workshop\\nlp_env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:791\u001b[39m, in \u001b[36mread_json\u001b[39m\u001b[34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient != \u001b[33m\"\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    789\u001b[39m     convert_axes = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m791\u001b[39m json_reader = \u001b[43mJsonReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43morient\u001b[49m\u001b[43m=\u001b[49m\u001b[43morient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_default_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    801\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    804\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    805\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    806\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    807\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    808\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\NLP-workshop\\nlp_env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:904\u001b[39m, in \u001b[36mJsonReader.__init__\u001b[39m\u001b[34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors, dtype_backend, engine)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = filepath_or_buffer\n\u001b[32m    903\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.engine == \u001b[33m\"\u001b[39m\u001b[33mujson\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data_from_filepath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28mself\u001b[39m._preprocess_data(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\NLP-workshop\\nlp_env\\Lib\\site-packages\\pandas\\io\\json\\_json.py:960\u001b[39m, in \u001b[36mJsonReader._get_data_from_filepath\u001b[39m\u001b[34m(self, filepath_or_buffer)\u001b[39m\n\u001b[32m    952\u001b[39m     filepath_or_buffer = \u001b[38;5;28mself\u001b[39m.handles.handle\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    954\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    955\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m filepath_or_buffer.lower().endswith(\n\u001b[32m   (...)\u001b[39m\u001b[32m    958\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[32m    959\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_or_buffer\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    962\u001b[39m     warnings.warn(\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPassing literal json to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mread_json\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    964\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mwill be removed in a future version. To read from a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    967\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    968\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File data/cc_gw_news_blogs_2021-10-01_2021-10-31.json does not exist"
     ]
    }
   ],
   "source": [
    "df_news = pd.read_json('data/cc_gw_news_blogs_2021-10-01_2021-10-31.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804609e4",
   "metadata": {},
   "source": [
    "You can investigate the columns of this dataframe to see what metadata is available for these articles, but for the purpose of this workshop, we're most interested in the `title` and `body` columns. They contain the article headline and body text respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.title[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a5831",
   "metadata": {},
   "source": [
    "This example shows one of the cases where you may wish to apply preprocessing to a text, that is the addition of less informative boiler plate text. In this case it is the addition of the source name after the title. We can easily partition the text on a particular string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c6a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.title[0].split(' - ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079479cc",
   "metadata": {},
   "source": [
    "A word of warning here - be careful if you apply such methods over larger volumes of text or from different sources as they may follow different linguistic styles.\n",
    "\n",
    "Another common cleaning step in NLP is to standardise the case in a text. Typically, NLP standardises to lowercase text. If this standardisation is not done, then the words `Biden`, `BIDEN`, and `biden` are considered different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5976f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.title[0].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea33975",
   "metadata": {},
   "source": [
    "Another aspect of text cleaning you may wish to apply considers the punctuation present in a text. Removing all punctuation from a string is easy using the `string` library. You'll see in the example below that this is not a foolproof step - removal of `'` has contracted `Nicaragua's` to `Nicaraguas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba02ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_news.title[1])\n",
    "print(df_news.title[1].translate(str.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6faef4de",
   "metadata": {},
   "source": [
    "The particular example listed above highlights another cleaning step you may wish to consider - stemming. Stemming is the process of identifying a word root from various derivative forms. For example, the stemmed forms of `fishes`, `fished` and `fishing` are all `fish`. As a result, you may find stemming a good way to group similar words. You'll see that in this case, the stemmer made all the text lower case automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32795aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "[ps.stem(w) for w in df_news.title[1].split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32123ed2",
   "metadata": {},
   "source": [
    "## A note on encoding\n",
    "\n",
    "Depending on the source of your text, you may find different text encodings, that is the way that the text is represented digitally. This is usually only problematic in the case of characters like emoji that are not included in the simplest alphabet. If you encounter such errors, try switching to a `utf-8` encoding when you read the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918093eb",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are a couple of exercises to apply the techniques discussed above.\n",
    "\n",
    "First, count the number of unique words in the titles of all articles in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92e1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4842edde",
   "metadata": {},
   "source": [
    "Which is the most common form of the root word `talk` in the titles? Hint: you may find the `collections` library useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f03fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3b58022",
   "metadata": {},
   "source": [
    "# Text searching\n",
    "\n",
    "Cleaning the text is designed to make searching and matching parts of the text easier by removing issues that prevent an intended match.\n",
    "\n",
    "There are a few methods for matching texts in Python. First, we can work on strings directly. Notice the case sensitivity - this is why lower case conversion is important for some NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72177a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_news.title[0])\n",
    "print('Trudeau' in df_news.title[0])\n",
    "print('trudeau' in df_news.title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2726c6",
   "metadata": {},
   "source": [
    "A more powerful means of matching patterns in text uses `regular expressions`. They allow you to define patterns of text to be matched (e.g. any number followed by a bracket such as `1)`) without exhaustively listing the possible combinations. The full power of regex queries are too detailed for this workshop, but I recommend [regex101](https://regex101.com/) as a useful site to define and test queries.\n",
    "\n",
    "Here's an example query to find GXX groups in texts. Regex queries are normally defined by blocks to match within square brackets `[]` with certain special character groupings predefined. The example below looks for `G` followed by at least one (`+`) digit character (`\\d`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = r'[G][\\d]+'\n",
    "re.findall(query,df_news.title[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fbff46",
   "metadata": {},
   "source": [
    "Applying this over all titles we find the following matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66202a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_groups = []\n",
    "for t in df_news.title:\n",
    "    g_groups += re.findall(query,t)\n",
    "print(collections.Counter(g_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93de6f",
   "metadata": {},
   "source": [
    "This example highlights an important caveat when defining text queries - check your data before and after returning the results. Here we were expecting the various levels of international economic fora like `G7` and `G20`. These weren't the only matches in our dataset - some of them are companies or other groups, but they are rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b1eda5",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Here are a couple of exercises on text matching. Note that these may be achievable with simple text matching, rather than regex but be aware of text inconsistencies.\n",
    "\n",
    "How many of the headlines mention Justin Trudeau?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef9af1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d152eee",
   "metadata": {},
   "source": [
    "How many of the headlines mention one of the G7 countries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a435a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "901d6e14-f5a7-4443-a024-63c703ef4fbf",
   "metadata": {},
   "source": [
    "How many of the articles reference one of the Conference of Parties (COP) events (in both headlines and bodies)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6d50d0-03ff-4de1-878a-9c8fe9e1dab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
